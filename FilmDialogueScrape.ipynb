{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from urllib.request import urlopen\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "import csv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = [] # set up initial data list for IMDB urls\n",
    "nndb_names = [] # set up initial name list to make NNDB search urls\n",
    "url_list = [] # set up initial url list for NNDB search urls\n",
    "link_list = [] # set up initial link list for NNDB actor urls \n",
    "race_list = [] # set up initial race list for actors' races\n",
    "\n",
    "url = \"http://www.imdb.com/name/\" # set up base url for IMDB search\n",
    "query_url = \"http://search.nndb.com/search/nndb.cgi?type=unspecified&query=\" # set up base url for NNDB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search actor via name ID (nm###...) and store actual name \n",
    "# ...takes in specific url and scrapes it\n",
    "\n",
    "def scrapeIMDB(url):\n",
    "    name_list = []\n",
    "    \n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        actor = soup.find(\"span\", attrs = {\"itemprop\": \"name\"}) # find & store actor's actual name\n",
    "        img_url = soup.find(\"img\", attrs = {\"id\": \"name-poster\"})\n",
    "        if img_url != None:\n",
    "            img_url = img_url.get(\"src\")\n",
    "            print(url[-9:] + \",\" + actor.text.strip() + \",\" + img_url) # print the actor's name id with their actual name in 'nm####,name' format for easy csv parsing printing is important because pooling always ends in error\n",
    "        else:\n",
    "            img_url = \"NA\"\n",
    "            print(url[-9:] + \",\" + actor.text.strip() + \",\" + img_url)  \n",
    "                                                                       \n",
    "\n",
    "        return name_list\n",
    "    except Exception as e: \n",
    "        print(\"shit stopped working at url: \" + url) # exception for cases where for some reason url invalid \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Documents/leftover_ids.csv', newline='') as csvfile: # open character id file\n",
    "    reader = csv.reader(csvfile, delimiter=' ', quotechar='|') # read it in, quotechar unnecessary\n",
    "    for row in reader:\n",
    "        data2.append(url + row[0]) # for each row in the csv, append the value in the first column to the end \n",
    "                                  # of the IMDB url, creating a list of accessible IMDB urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_size = 3000 # declare rough size of list to deal with \n",
    "\n",
    "num_workers = 4 * 2 # should be using the number of cpu cores in the computer via \n",
    "                    # some reference, but macbooks have 4 cores, so this works\n",
    "    \n",
    "chunksize = list_size // num_workers * 4  # Try to get a good chunksize. You're probably going to have \n",
    "                                          # to tweak this, though. Try smaller and lower values and see \n",
    "                                          # how performance changes.\n",
    "        \n",
    "pool = Pool(num_workers) # don't fully understand this call, but it allows the pool.map function to run\n",
    "\n",
    "actors = pool.map(scrapeIMDB, data2) # calls function 'scrapeIMDB' using elements of list 'data'\n",
    "                                    # won't end properly, but that's why we print the output\n",
    "    \n",
    "# COPY AND PASTE OUTPUT into Actors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes in NNDB search url, and if search was a match, records the\n",
    "# first link in the database (to the matched name). If no match, records NA value\n",
    "\n",
    "def scrapeLink(url):\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        global link_list # calling in link_list in order to modify, but hasn't worked yet\n",
    "        temp = soup.find(\"p\")\n",
    "        \n",
    "        if temp is not None: # must check if not none even though you have try/exception — makes sure you \n",
    "                             # perform function on empty value \n",
    "            \n",
    "            if soup.find(\"p\").contents[0] == '\\nNo names match your query ':\n",
    "                link_list.append(\"NA\")\n",
    "                print(url + \",\" + \"NA\") # print the url in order to record characters not on NNDB \n",
    "            else:\n",
    "                link = soup.findAll(\"a\", href = True)[2]['href'] # find link to actor's actual NNDB page\n",
    "                link_list.append(link) # append link to link_list\n",
    "\n",
    "                print(url + \",\" + link) # print url in order to stay consistent\n",
    "        else:\n",
    "            print(url + \",\" + \"NA\") # again, print url to record which links don't work\n",
    "\n",
    "        return link_list\n",
    "    except Exception as e:\n",
    "        print(\"shit stopped working at url: \" + url)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeRace(url):\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        global race_list # global to preserve modification outside of function\n",
    "\n",
    "        name = soup.find(\"font\", attrs = {\"size\": \"+3\"}) # get name of actor to tie name to race\n",
    "        race = soup.find(\"b\", text=\"Race or Ethnicity:\") # get field for race of actor\n",
    "        \n",
    "        if name is not None and race is not None:\n",
    "            name = name.text.strip() # get name in string form\n",
    "            race = race.next_sibling # get race of actor\n",
    "            race_list.append(race) # append race to race_list\n",
    "\n",
    "            print(name + \",\" + race) # print name and race to log race of each actor in csv parseable format\n",
    "\n",
    "        return race_list\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"shit stopped working at url: \" + url)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need each individual word separate for NNDB url\n",
    "with open('Downloads/Actors.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        nndb_names.append(row[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing empty list items so url structure makes sense \n",
    "# (e.g. \"...michael+keaton\" instead of \"...michael+keaton+\")\n",
    "\n",
    "for i in range(0, len(nndb_names)):\n",
    "    nndb_names[i] = list(filter(None, nndb_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(nndb_names)): # cycle through the entire list of actors to search on NNDB\n",
    "    \n",
    "    for j in range(0, len(nndb_names[i])): # search through list of first, middle, & last names for each actor\n",
    "        \n",
    "        if len(nndb_names[i]) == 3 and j == 0: # if we're at the first name of a person with f, m, & l\n",
    "            page3 = query_url + nndb_names[i][j] + \"+\" + nndb_names[i][j+1] + \"+\"  # add first two names to url +\n",
    "            \n",
    "        elif j == 0: # if we're at the first name of a person with just f & l\n",
    "            page3 = query_url + nndb_names[i][j] + \"+\" # add first name +\n",
    "            \n",
    "        elif j == len(nndb_names[i])-1: # if we're a the last name of a person\n",
    "            page3 = page3 + nndb_names[i][j] # add their last name to the url\n",
    "            \n",
    "    url_list.append(page3) # append completed urls to list of searchable NNDB query urls\n",
    "    page3 = \"\" # set page equal to none again since every name ends with a full page3 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, item in enumerate(url_list): # this is to remove pesky characters that somehow enter the fold\n",
    "                                      # and fuck with your program — it replaces any url containing one \n",
    "                                      # of these characters with a searchable url that will return an NA value\n",
    "    if 'â' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '€' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '¦' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif 'Ã' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '¥' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '«' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '¢' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif '¾' in item:\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Dennis+North'\n",
    "    elif item == 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Michael+PeÃ‘a':\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Michael+Pena'\n",
    "    elif item == 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Ren_e+Zellweger':\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Renee+Zellweger'\n",
    "    elif item == 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=RZA+':\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=RZA'\n",
    "    elif item == 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Juan+Carlos+Hernâ€¦ndez':\n",
    "        url_list[idx] = 'http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Juan+Carlos+Hernandez'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = 33944 \n",
    "num_workers = 4 * 2\n",
    "chunksize = list_size // num_workers * 5\n",
    "pool = Pool(num_workers)\n",
    "\n",
    "links = pool.map(scrapeLink, url_list)\n",
    "\n",
    "# COPY AND PASTE OUTPUT into NNDB_Actors.csv (should change name to something more accurate, like NNDB_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Documents/NNDB_Actors.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        link_list.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = 12000 \n",
    "num_workers = 4 * 2\n",
    "chunksize = list_size // num_workers * 4\n",
    "\n",
    "pool2 = Pool(num_workers)\n",
    "races = pool2.map(scrapeRace, link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to search actor via name ID (nm###...) and store actual name \n",
    "# ...takes in specific url and scrapes it\n",
    "\n",
    "def scrapeInfo(url):\n",
    "    try:\n",
    "        page = urlopen(url)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        page.close()\n",
    "\n",
    "        info_list = []\n",
    "        \n",
    "        gross = soup.find(\"h4\", text = \"Gross USA:\")\n",
    "        if (gross != None):\n",
    "            gross = gross.next_sibling\n",
    "        production = soup.find(\"h4\", text = \"Production Co:\")\n",
    "        if (production != None):\n",
    "            production = production.next_sibling.next_sibling.text.strip()\n",
    "        genre = soup.find(\"div\", attrs = {\"itemprop\": \"genre\"}) \n",
    "        if (genre != None):\n",
    "            genre = genre.text.strip()\n",
    "            \n",
    "        if (gross != None and production != None and genre != None):\n",
    "            print(url[-9:] + \",\" + gross.encode(\"unicode_escape\").decode(\"utf-8\") + \",\" + production.encode(\"unicode_escape\").decode(\"utf-8\") + \",\" + genre.encode(\"unicode_escape\").decode(\"utf-8\")) # print the actor's name id with their actual name in\n",
    "                                                  \n",
    "\n",
    "        return info_list\n",
    "    except Exception as e: \n",
    "        print(\"shit stopped working at url: \" + url) # exception for cases where for some reason url invalid \n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_size = 25000 \n",
    "num_workers = 4 * 2\n",
    "chunksize = list_size // num_workers * 4\n",
    "\n",
    "pool2 = Pool(num_workers)\n",
    "races = pool2.map(scrapeInfo, title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.imdb.com/title/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "\n",
    "with open('Documents/TitleList.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        title_list.append(url + row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for i in range (0, 3):\n",
    "    page2 = urlopen(url_list[i])\n",
    "    soup2 = BeautifulSoup(page2, \"html.parser\")\n",
    "    page2.close()\n",
    "\n",
    "    if soup2.find(\"p\").contents[0] == '\\nNo names match your query ':\n",
    "        temp.append(\"NA\")\n",
    "    else:\n",
    "        link = soup2.findAll(\"a\", href = True)[2]['href']\n",
    "        temp.append(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_list = []\n",
    "\n",
    "for i in range (0, len(temp)):\n",
    "    if temp[i] != \"NA\":\n",
    "        page5 = urlopen(temp[i])\n",
    "        soup5 = BeautifulSoup(page5, \"html.parser\")\n",
    "        page5.close()\n",
    "        \n",
    "        name = soup5.find(\"font\", attrs = {\"size\": \"+3\"}).text.strip()\n",
    "        race = soup5.find(\"b\", text=\"Race or Ethnicity:\").next_sibling\n",
    "        race_list.append(race)\n",
    "    else: \n",
    "        race_list.append(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nndb_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulimit -n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page5 = urlopen('http://search.nndb.com/search/nndb.cgi?type=unspecified&query=Waldemar+Kobus')\n",
    "soup5 = BeautifulSoup(page5, \"html.parser\")\n",
    "page5.close()\n",
    "        \n",
    "race = soup5.find(\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list[-0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Documents/Actors.csv', 'a') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"name\"]) # write csv file headers\n",
    "    for i in range (0, len(actors)):\n",
    "        writer.writerow([actors[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
